{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMPO pour ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaN avant : 23685032\n",
      "Nombre de NaN après : 0\n",
      "Nombre de NaN dans vomecrtyT avant : 24555776\n",
      "Nombre de NaN dans vozocrtxT avant : 24555776\n",
      "Nombre de NaN dans sossheig avant : 23685032\n",
      "Nombre de NaN dans votemper avant : 23685032\n",
      "Nombre de NaN dans vomecrtyT après : 0\n",
      "Nombre de NaN dans vozocrtxT après : 0\n",
      "Nombre de NaN dans sossheig après : 0\n",
      "Nombre de NaN dans votemper après : 0\n",
      "Nombre de NaN dans eddies après remplacement : 0\n",
      "Nombre de NaN dans vomecrtyT après remplacement : 0\n",
      "Nombre de NaN dans vozocrtxT après remplacement : 0\n",
      "Nombre de NaN dans sossheig après remplacement : 0\n",
      "Nombre de NaN dans votemper après remplacement : 0\n",
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:    (time: 284, latitude: 357, longitude: 717)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 2kB 2015-01-02T12:00:00 ... 2015-12-25T1...\n",
      "  * latitude   (latitude) float32 1kB -1.084e-19 2.967 0.0 ... 3.099 0.0\n",
      "  * longitude  (longitude) float32 3kB 3.689e+19 -3.352 0.0 ... -3.221 0.0\n",
      "    deptht     float32 4B 0.494\n",
      "Data variables:\n",
      "    eddies     (time, latitude, longitude) float64 582MB 999.0 999.0 ... 0.0 0.0\n",
      "    vomecrtyT  (time, latitude, longitude) float32 291MB 0.5162 ... 0.5587\n",
      "    vozocrtxT  (time, latitude, longitude) float32 291MB 0.4055 ... 0.4137\n",
      "    sossheig   (time, latitude, longitude) float32 291MB 0.4632 ... 0.1708\n",
      "    votemper   (time, latitude, longitude) float32 291MB 0.06519 ... 0.2978\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import cmocean\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "\n",
    "eddies_train = xr.open_dataset(\"data/eddies_train.nc\")\n",
    "OSSE_train = xr.open_dataset(\"data/OSSE_U_V_SLA_SST_train.nc\")\n",
    "\n",
    "# Remplacer les valeurs de \"nan\", \"NaN\", \"NAN\" par np.nan\n",
    "eddies_train[\"eddies\"] = eddies_train[\"eddies\"].where(~eddies_train[\"eddies\"].isin([\"nan\", \"NaN\", \"NAN\", \"NaN \"]), np.nan)\n",
    "\n",
    "# Vérifier le nombre de NaN avant modification\n",
    "print(\"Nombre de NaN avant :\", np.isnan(eddies_train[\"eddies\"]).sum().values)\n",
    "\n",
    "# Remplacer les NaN par 999\n",
    "eddies_train[\"eddies\"] = eddies_train[\"eddies\"].fillna(999)\n",
    "\n",
    "# Vérifier qu'il n'y a plus de NaN\n",
    "print(\"Nombre de NaN après :\", np.isnan(eddies_train[\"eddies\"]).sum().values)\n",
    "\n",
    "# Vérifier et remplacer les occurrences de \"nan\", \"NaN\", \"NAN\", \"NaN \" par np.nan dans chaque variable\n",
    "for var in OSSE_train.data_vars:\n",
    "    # Remplacer les valeurs de \"nan\", \"NaN\", \"NAN\", \"NaN \" par np.nan\n",
    "    OSSE_train[var] = OSSE_train[var].where(~OSSE_train[var].isin([\"nan\", \"NaN\", \"NAN\", \"NaN \"]), np.nan)\n",
    "    \n",
    "    # Vérifier le nombre de NaN dans chaque variable avant modification\n",
    "    print(f\"Nombre de NaN dans {var} avant :\", np.isnan(OSSE_train[var]).sum().values)\n",
    "\n",
    "# Remplacer les NaN par 999 pour chaque variable\n",
    "for var in OSSE_train.data_vars:\n",
    "    OSSE_train[var] = OSSE_train[var].fillna(0)\n",
    "\n",
    "# Vérifier qu'il n'y a plus de NaN\n",
    "for var in OSSE_train.data_vars:\n",
    "    print(f\"Nombre de NaN dans {var} après :\", np.isnan(OSSE_train[var]).sum().values)\n",
    "\n",
    "# Renommer `time_counter` en `time`\n",
    "OSSE_train = OSSE_train.rename({\"time_counter\": \"time\"})\n",
    "\n",
    "# Supprimer l'ancienne dimension `time_counter` si elle est encore présente\n",
    "if \"time_counter\" in OSSE_train.coords:\n",
    "    OSSE_train = OSSE_train.drop_vars(\"time_counter\")\n",
    "\n",
    "# Fusionner les datasets\n",
    "merged_ds = xr.merge([eddies_train, OSSE_train])\n",
    "merged_ds\n",
    "\n",
    "# Remplacer les occurrences de chaînes 'nan', 'NaN', 'NAN' par np.nan\n",
    "merged_ds = merged_ds.apply(lambda x: x.where(~x.isin(['nan', 'NaN', 'NAN']), np.nan))\n",
    "\n",
    "# Remplacer toutes les occurrences de NaN (y compris celles qui sont des np.nan maintenant) par 0\n",
    "merged_ds = merged_ds.fillna(0)\n",
    "\n",
    "# Vérifier le nombre de NaN dans chaque variable après modification\n",
    "for var in merged_ds.data_vars:\n",
    "    print(f\"Nombre de NaN dans {var} après remplacement :\", np.isnan(merged_ds[var]).sum().values)\n",
    "    \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "vars_to_normalize = [\"vomecrtyT\", \"vozocrtxT\", \"sossheig\", \"votemper\"]\n",
    "\n",
    "def min_max_scale(da):\n",
    "    scaler = MinMaxScaler()\n",
    "    flat_data = da.values.reshape(-1, 1)\n",
    "    scaled = scaler.fit_transform(flat_data).reshape(da.shape)\n",
    "    return xr.DataArray(scaled, coords=da.coords, dims=da.dims, attrs=da.attrs)\n",
    "\n",
    "normalized_ds = merged_ds.copy()\n",
    "for var in vars_to_normalize:\n",
    "    normalized_ds[var] = min_max_scale(merged_ds[var])\n",
    "\n",
    "\n",
    "print(normalized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m3/k2rlmgps1yg0xlxmdh61l9pw0000gn/T/ipykernel_91255/246395216.py:20: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  return self.data.dims[\"time\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisé : mps\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "\n",
    "ds = normalized_ds\n",
    "# Définition de la classe OsseDataset\n",
    "class OsseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (xarray.Dataset): Doit contenir les variables \n",
    "                vozocrtxT, vomecrtyT, sossheig, votemper, eddies.\n",
    "            transform (callable, optional): Transformation à appliquer.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.dims[\"time\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extraction des variables (chaque variable est de forme (H, W))\n",
    "        U = self.data.vozocrtxT.values[idx, :, :]\n",
    "        V = self.data.vomecrtyT.values[idx, :, :]\n",
    "        sossheig = self.data.sossheig.values[idx, :, :]\n",
    "        votemper = self.data.votemper.values[idx, :, :]\n",
    "        eddies = self.data.eddies.values[idx, :, :]\n",
    "\n",
    "        # Conversion en tenseurs PyTorch\n",
    "        U = torch.tensor(U, dtype=torch.float32)\n",
    "        V = torch.tensor(V, dtype=torch.float32)\n",
    "        sossheig = torch.tensor(sossheig, dtype=torch.float32)\n",
    "        votemper = torch.tensor(votemper, dtype=torch.float32)\n",
    "        targets = torch.tensor(eddies, dtype=torch.float32)\n",
    "        \n",
    "        # Concaténation des tenseurs le long de la dimension des canaux\n",
    "        # Le résultat aura la forme (4, H, W)\n",
    "        inputs_tensor = torch.stack([U, V, sossheig, votemper], dim=0)\n",
    "        \n",
    "        # Appliquer une transformation si nécessaire\n",
    "        if self.transform:\n",
    "            sample = {'inputs': inputs_tensor, 'targets': targets}\n",
    "            sample = self.transform(sample)\n",
    "            return sample['inputs'], sample['targets']\n",
    "\n",
    "        return inputs_tensor, targets\n",
    "\n",
    "# Création de l'instance du dataset\n",
    "dataset = OsseDataset(ds, transform=None)\n",
    "\n",
    "# Division en ensembles d'entraînement et de validation (80%/20%)\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "b_size = 8\n",
    "\n",
    "# Création des DataLoader pour l'entraînement et la validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=b_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=b_size, shuffle=False)\n",
    "\n",
    "\n",
    "class EddyPredictorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EddyPredictorCNN, self).__init__()\n",
    "        # Ici on attend une entrée avec 4 canaux, et on souhaite une sortie avec 1 canal de même taille (pour la régression par pixel)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)  # Renvoie une carte de sortie d'un canal\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers(x)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc1 = self.conv_block(4, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.center = self.conv_block(512, 1024)\n",
    "        self.dec4 = self.conv_block(1024 + 512, 512)\n",
    "        self.dec3 = self.conv_block(512 + 256, 256)\n",
    "        self.dec2 = self.conv_block(256 + 128, 128)\n",
    "        self.dec1 = self.conv_block(128 + 64, 64)\n",
    "        self.final = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(F.max_pool2d(enc1, 2))\n",
    "        enc3 = self.enc3(F.max_pool2d(enc2, 2))\n",
    "        enc4 = self.enc4(F.max_pool2d(enc3, 2))\n",
    "        center = self.center(F.max_pool2d(enc4, 2))\n",
    "        \n",
    "        # Spécifier la taille exacte pour l'interpolation\n",
    "        up_center = F.interpolate(center, size=enc4.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec4 = self.dec4(torch.cat([up_center, enc4], dim=1))\n",
    "        \n",
    "        up_dec4 = F.interpolate(dec4, size=enc3.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec3 = self.dec3(torch.cat([up_dec4, enc3], dim=1))\n",
    "        \n",
    "        up_dec3 = F.interpolate(dec3, size=enc2.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec2 = self.dec2(torch.cat([up_dec3, enc2], dim=1))\n",
    "        \n",
    "        up_dec2 = F.interpolate(dec2, size=enc1.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec1 = self.dec1(torch.cat([up_dec2, enc1], dim=1))\n",
    "        \n",
    "        final = self.final(dec1)\n",
    "        return final\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def validation(net):\n",
    "    net.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs_tensor = inputs.to(device)\n",
    "            labels = labels.to(device)  # Envoi des labels sur GPU\n",
    "\n",
    "            # Prédictions et calcul de la perte\n",
    "            outputs = net(inputs_tensor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(val_loader)  # Moyenne des pertes\n",
    "    net.train()\n",
    "    return valid_loss\n",
    "\n",
    "def train(model, num_epochs, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_history = []\n",
    "    valid_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs_tensor = inputs.to(device)\n",
    "            targets = targets.to(device)  # Envoi sur GPU           \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_tensor)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)  # Moyenne de la perte sur chaque image\n",
    "        valid_loss = validation(model)  # Calcul validation loss\n",
    "        train_history.append(train_loss) # On ajoute tout aux listes pour tracer les résultats \n",
    "        valid_history.append(valid_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch:02d}: train loss {train_loss:.5f}, validation loss {valid_loss:.5f}')\n",
    "    \n",
    "    torch.save(model.state_dict(), \"eddy_predictor.pth\")\n",
    "\n",
    "    return train_history, valid_history\n",
    "    \n",
    "\n",
    "# On se met sur les GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Device utilisé :\", device)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    " \n",
    "# Initialisation du modèle\n",
    "model = UNet().to(device)  # Ajoutez les parenthèses pour instancier le modèle\n",
    "\n",
    "train_history, valid_history = train(model, 1, device)\n",
    "#plot_train_val(train_history, valid_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m3/k2rlmgps1yg0xlxmdh61l9pw0000gn/T/ipykernel_92248/2759806250.py:15: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  return self.data.dims[\"time\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisé : cpu\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "\n",
    "ds = normalized_ds\n",
    "\n",
    "# Définition de la classe OsseDataset\n",
    "class OsseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.dims[\"time\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        U = self.data.vozocrtxT.values[idx, :, :]\n",
    "        V = self.data.vomecrtyT.values[idx, :, :]\n",
    "        sossheig = self.data.sossheig.values[idx, :, :]\n",
    "        votemper = self.data.votemper.values[idx, :, :]\n",
    "        eddies = self.data.eddies.values[idx, :, :]\n",
    "\n",
    "        U = torch.tensor(U, dtype=torch.float32)\n",
    "        V = torch.tensor(V, dtype=torch.float32)\n",
    "        sossheig = torch.tensor(sossheig, dtype=torch.float32)\n",
    "        votemper = torch.tensor(votemper, dtype=torch.float32)\n",
    "        targets = torch.tensor(eddies, dtype=torch.long)  # Correction ici\n",
    "\n",
    "        inputs_tensor = torch.stack([U, V, sossheig, votemper], dim=0)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = {'inputs': inputs_tensor, 'targets': targets}\n",
    "            sample = self.transform(sample)\n",
    "            return sample['inputs'], sample['targets']\n",
    "\n",
    "        return inputs_tensor, targets\n",
    "\n",
    "# Création du dataset et des DataLoader\n",
    "# Remplacer len(ds) par len(ds.time) pour obtenir la dimension appropriée.\n",
    "train_size = int(0.8 * len(ds.time))\n",
    "val_size = len(ds.time) - train_size\n",
    "train_dataset, val_dataset = random_split(OsseDataset(ds), [train_size, val_size])\n",
    "\n",
    "b_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=b_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=b_size, shuffle=False)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc1 = self.conv_block(4, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.center = self.conv_block(512, 1024)\n",
    "        self.dec4 = self.conv_block(1024 + 512, 512)\n",
    "        self.dec3 = self.conv_block(512 + 256, 256)\n",
    "        self.dec2 = self.conv_block(256 + 128, 128)\n",
    "        self.dec1 = self.conv_block(128 + 64, 64)\n",
    "        self.final = nn.Conv2d(64, 3, kernel_size=1)  # Correction ici\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(nn.functional.max_pool2d(enc1, 2))\n",
    "        enc3 = self.enc3(nn.functional.max_pool2d(enc2, 2))\n",
    "        enc4 = self.enc4(nn.functional.max_pool2d(enc3, 2))\n",
    "        center = self.center(nn.functional.max_pool2d(enc4, 2))\n",
    "        \n",
    "        up_center = nn.functional.interpolate(center, size=enc4.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec4 = self.dec4(torch.cat([up_center, enc4], dim=1))\n",
    "        up_dec4 = nn.functional.interpolate(dec4, size=enc3.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec3 = self.dec3(torch.cat([up_dec4, enc3], dim=1))\n",
    "        up_dec3 = nn.functional.interpolate(dec3, size=enc2.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec2 = self.dec2(torch.cat([up_dec3, enc2], dim=1))\n",
    "        up_dec2 = nn.functional.interpolate(dec2, size=enc1.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec1 = self.dec1(torch.cat([up_dec2, enc1], dim=1))\n",
    "        final = self.final(dec1)\n",
    "        return final\n",
    "\n",
    "def validation(net):\n",
    "    net.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs_tensor = inputs.to(device)\n",
    "            labels = labels.to(device, dtype=torch.long)  # Correction ici\n",
    "            outputs = net(inputs_tensor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    valid_loss /= len(val_loader)\n",
    "    net.train()\n",
    "    return valid_loss\n",
    "\n",
    "def train(model, num_epochs, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs_tensor = inputs.to(device)\n",
    "            targets = targets.to(device, dtype=torch.long)  # Correction ici\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_tensor)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        valid_loss = validation(model)\n",
    "        print(f'Epoch {epoch+1}: train loss {train_loss:.5f}, validation loss {valid_loss:.5f}')\n",
    "    \n",
    "    torch.save(model.state_dict(), \"eddy_predictor.pth\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "print(\"Device utilisé :\", device)\n",
    "model = UNet().to(device)\n",
    "train(model, 1, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi1supaero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
