{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMPO pour ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaN avant : 23685032\n",
      "Nombre de NaN après : 0\n",
      "Nombre de NaN dans vomecrtyT avant : 24555776\n",
      "Nombre de NaN dans vozocrtxT avant : 24555776\n",
      "Nombre de NaN dans sossheig avant : 23685032\n",
      "Nombre de NaN dans votemper avant : 23685032\n",
      "Nombre de NaN dans vomecrtyT après : 0\n",
      "Nombre de NaN dans vozocrtxT après : 0\n",
      "Nombre de NaN dans sossheig après : 0\n",
      "Nombre de NaN dans votemper après : 0\n",
      "Nombre de NaN dans eddies après remplacement : 0\n",
      "Nombre de NaN dans vomecrtyT après remplacement : 0\n",
      "Nombre de NaN dans vozocrtxT après remplacement : 0\n",
      "Nombre de NaN dans sossheig après remplacement : 0\n",
      "Nombre de NaN dans votemper après remplacement : 0\n",
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:    (time: 284, latitude: 357, longitude: 717)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 2kB 2015-01-02T12:00:00 ... 2015-12-25T1...\n",
      "  * latitude   (latitude) float32 1kB -1.084e-19 2.967 0.0 ... 3.099 0.0\n",
      "  * longitude  (longitude) float32 3kB 3.689e+19 -3.352 0.0 ... -3.221 0.0\n",
      "    deptht     float32 4B 0.494\n",
      "Data variables:\n",
      "    eddies     (time, latitude, longitude) float64 582MB 999.0 999.0 ... 0.0 0.0\n",
      "    vomecrtyT  (time, latitude, longitude) float32 291MB 0.5162 ... 0.5587\n",
      "    vozocrtxT  (time, latitude, longitude) float32 291MB 0.4055 ... 0.4137\n",
      "    sossheig   (time, latitude, longitude) float32 291MB 0.4632 ... 0.1708\n",
      "    votemper   (time, latitude, longitude) float32 291MB 0.06519 ... 0.2978\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import cmocean\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "\n",
    "eddies_train = xr.open_dataset(\"data/eddies_train.nc\")\n",
    "OSSE_train = xr.open_dataset(\"data/OSSE_U_V_SLA_SST_train.nc\")\n",
    "\n",
    "# Remplacer les valeurs de \"nan\", \"NaN\", \"NAN\" par np.nan\n",
    "eddies_train[\"eddies\"] = eddies_train[\"eddies\"].where(~eddies_train[\"eddies\"].isin([\"nan\", \"NaN\", \"NAN\", \"NaN \"]), np.nan)\n",
    "\n",
    "# Vérifier le nombre de NaN avant modification\n",
    "print(\"Nombre de NaN avant :\", np.isnan(eddies_train[\"eddies\"]).sum().values)\n",
    "\n",
    "# Remplacer les NaN par 999\n",
    "eddies_train[\"eddies\"] = eddies_train[\"eddies\"].fillna(999)\n",
    "\n",
    "# Vérifier qu'il n'y a plus de NaN\n",
    "print(\"Nombre de NaN après :\", np.isnan(eddies_train[\"eddies\"]).sum().values)\n",
    "\n",
    "# Vérifier et remplacer les occurrences de \"nan\", \"NaN\", \"NAN\", \"NaN \" par np.nan dans chaque variable\n",
    "for var in OSSE_train.data_vars:\n",
    "    # Remplacer les valeurs de \"nan\", \"NaN\", \"NAN\", \"NaN \" par np.nan\n",
    "    OSSE_train[var] = OSSE_train[var].where(~OSSE_train[var].isin([\"nan\", \"NaN\", \"NAN\", \"NaN \"]), np.nan)\n",
    "    \n",
    "    # Vérifier le nombre de NaN dans chaque variable avant modification\n",
    "    print(f\"Nombre de NaN dans {var} avant :\", np.isnan(OSSE_train[var]).sum().values)\n",
    "\n",
    "# Remplacer les NaN par 999 pour chaque variable\n",
    "for var in OSSE_train.data_vars:\n",
    "    OSSE_train[var] = OSSE_train[var].fillna(0)\n",
    "\n",
    "# Vérifier qu'il n'y a plus de NaN\n",
    "for var in OSSE_train.data_vars:\n",
    "    print(f\"Nombre de NaN dans {var} après :\", np.isnan(OSSE_train[var]).sum().values)\n",
    "\n",
    "# Renommer `time_counter` en `time`\n",
    "OSSE_train = OSSE_train.rename({\"time_counter\": \"time\"})\n",
    "\n",
    "# Supprimer l'ancienne dimension `time_counter` si elle est encore présente\n",
    "if \"time_counter\" in OSSE_train.coords:\n",
    "    OSSE_train = OSSE_train.drop_vars(\"time_counter\")\n",
    "\n",
    "# Fusionner les datasets\n",
    "merged_ds = xr.merge([eddies_train, OSSE_train])\n",
    "merged_ds\n",
    "\n",
    "# Remplacer les occurrences de chaînes 'nan', 'NaN', 'NAN' par np.nan\n",
    "merged_ds = merged_ds.apply(lambda x: x.where(~x.isin(['nan', 'NaN', 'NAN']), np.nan))\n",
    "\n",
    "# Remplacer toutes les occurrences de NaN (y compris celles qui sont des np.nan maintenant) par 0\n",
    "merged_ds = merged_ds.fillna(0)\n",
    "\n",
    "# Vérifier le nombre de NaN dans chaque variable après modification\n",
    "for var in merged_ds.data_vars:\n",
    "    print(f\"Nombre de NaN dans {var} après remplacement :\", np.isnan(merged_ds[var]).sum().values)\n",
    "    \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "vars_to_normalize = [\"vomecrtyT\", \"vozocrtxT\", \"sossheig\", \"votemper\"]\n",
    "\n",
    "def min_max_scale(da):\n",
    "    scaler = MinMaxScaler()\n",
    "    flat_data = da.values.reshape(-1, 1)\n",
    "    scaled = scaler.fit_transform(flat_data).reshape(da.shape)\n",
    "    return xr.DataArray(scaled, coords=da.coords, dims=da.dims, attrs=da.attrs)\n",
    "\n",
    "normalized_ds = merged_ds.copy()\n",
    "for var in vars_to_normalize:\n",
    "    normalized_ds[var] = min_max_scale(merged_ds[var])\n",
    "\n",
    "\n",
    "print(normalized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m3/k2rlmgps1yg0xlxmdh61l9pw0000gn/T/ipykernel_93814/3434310910.py:20: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  return self.data.dims[\"time\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisé : mps\n",
      "Epoch 00: train loss 0.00000, validation loss 0.00000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "\n",
    "ds = normalized_ds\n",
    "# Définition de la classe OsseDataset\n",
    "class OsseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (xarray.Dataset): Doit contenir les variables \n",
    "                vozocrtxT, vomecrtyT, sossheig, votemper, eddies.\n",
    "            transform (callable, optional): Transformation à appliquer.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.dims[\"time\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extraction des variables (chaque variable est de forme (H, W))\n",
    "        U = self.data.vozocrtxT.values[idx, :, :]\n",
    "        V = self.data.vomecrtyT.values[idx, :, :]\n",
    "        sossheig = self.data.sossheig.values[idx, :, :]\n",
    "        votemper = self.data.votemper.values[idx, :, :]\n",
    "        eddies = self.data.eddies.values[idx, :, :]\n",
    "\n",
    "        # Conversion en tenseurs PyTorch\n",
    "        U = torch.tensor(U, dtype=torch.float32)\n",
    "        V = torch.tensor(V, dtype=torch.float32)\n",
    "        sossheig = torch.tensor(sossheig, dtype=torch.float32)\n",
    "        votemper = torch.tensor(votemper, dtype=torch.float32)\n",
    "        targets = torch.tensor(eddies, dtype=torch.float32)\n",
    "        \n",
    "        # Concaténation des tenseurs le long de la dimension des canaux\n",
    "        # Le résultat aura la forme (4, H, W)\n",
    "        inputs_tensor = torch.stack([U, V, sossheig, votemper], dim=0)\n",
    "        \n",
    "        # Appliquer une transformation si nécessaire\n",
    "        if self.transform:\n",
    "            sample = {'inputs': inputs_tensor, 'targets': targets}\n",
    "            sample = self.transform(sample)\n",
    "            return sample['inputs'], sample['targets']\n",
    "\n",
    "        return inputs_tensor, targets\n",
    "\n",
    "# Création de l'instance du dataset\n",
    "dataset = OsseDataset(ds, transform=None)\n",
    "\n",
    "# Division en ensembles d'entraînement et de validation (80%/20%)\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "b_size = 8\n",
    "\n",
    "# Création des DataLoader pour l'entraînement et la validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=b_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=b_size, shuffle=False)\n",
    "\n",
    "\n",
    "class EddyPredictorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EddyPredictorCNN, self).__init__()\n",
    "        # Ici on attend une entrée avec 4 canaux, et on souhaite une sortie avec 1 canal de même taille (pour la régression par pixel)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            #nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)  # Renvoie une carte de sortie d'un canal\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc1 = self.conv_block(4, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.center = self.conv_block(512, 1024)\n",
    "        self.dec4 = self.conv_block(1024 + 512, 512)\n",
    "        self.dec3 = self.conv_block(512 + 256, 256)\n",
    "        self.dec2 = self.conv_block(256 + 128, 128)\n",
    "        self.dec1 = self.conv_block(128 + 64, 64)\n",
    "        self.final = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(F.max_pool2d(enc1, 2))\n",
    "        enc3 = self.enc3(F.max_pool2d(enc2, 2))\n",
    "        enc4 = self.enc4(F.max_pool2d(enc3, 2))\n",
    "        center = self.center(F.max_pool2d(enc4, 2))\n",
    "        \n",
    "        # Spécifier la taille exacte pour l'interpolation\n",
    "        up_center = F.interpolate(center, size=enc4.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec4 = self.dec4(torch.cat([up_center, enc4], dim=1))\n",
    "        \n",
    "        up_dec4 = F.interpolate(dec4, size=enc3.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec3 = self.dec3(torch.cat([up_dec4, enc3], dim=1))\n",
    "        \n",
    "        up_dec3 = F.interpolate(dec3, size=enc2.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec2 = self.dec2(torch.cat([up_dec3, enc2], dim=1))\n",
    "        \n",
    "        up_dec2 = F.interpolate(dec2, size=enc1.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        dec1 = self.dec1(torch.cat([up_dec2, enc1], dim=1))\n",
    "        \n",
    "        final = self.final(dec1)\n",
    "        return final\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def validation(net):\n",
    "    net.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs_tensor = inputs.to(device)\n",
    "            labels = labels.to(device)  # Envoi des labels sur GPU\n",
    "\n",
    "            # Prédictions et calcul de la perte\n",
    "            outputs = net(inputs_tensor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(val_loader)  # Moyenne des pertes\n",
    "    net.train()\n",
    "    return valid_loss\n",
    "\n",
    "def train(model, num_epochs, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=999)\n",
    "    \n",
    "    train_history = []\n",
    "    valid_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs_tensor = inputs.to(device)\n",
    "            targets = targets.to(device)  # Envoi sur GPU           \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_tensor)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)  # Moyenne de la perte sur chaque image\n",
    "        valid_loss = validation(model)  # Calcul validation loss\n",
    "        train_history.append(train_loss) # On ajoute tout aux listes pour tracer les résultats \n",
    "        valid_history.append(valid_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch:02d}: train loss {train_loss:.5f}, validation loss {valid_loss:.5f}')\n",
    "    \n",
    "    torch.save(model.state_dict(), \"eddy_predictor.pth\")\n",
    "\n",
    "    return train_history, valid_history\n",
    "    \n",
    "\n",
    "# On se met sur les GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Device utilisé :\", device)\n",
    "\n",
    " \n",
    "# Initialisation du modèle\n",
    "model = EddyPredictorCNN()\n",
    "model = model.to(device) # Ajoutez les parenthèses pour instancier le modèle\n",
    "\n",
    "train_history, valid_history = train(model, 1, device)\n",
    "#plot_train_val(train_history, valid_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m3/k2rlmgps1yg0xlxmdh61l9pw0000gn/T/ipykernel_93814/3223464861.py:22: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  return self.data.dims[\"time\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisé : mps\n",
      "Epoch 00: train loss 0.94684, validation loss 0.79238\n",
      "Epoch 01: train loss 0.74722, validation loss 0.73651\n",
      "Epoch 02: train loss 0.72425, validation loss 0.71653\n",
      "Epoch 03: train loss 0.70696, validation loss 0.70212\n",
      "Epoch 04: train loss 0.69516, validation loss 0.69290\n",
      "Epoch 05: train loss 0.68946, validation loss 0.68780\n",
      "Epoch 06: train loss 0.68558, validation loss 0.68485\n",
      "Epoch 07: train loss 0.68360, validation loss 0.68267\n",
      "Epoch 08: train loss 0.68192, validation loss 0.68120\n",
      "Epoch 09: train loss 0.68045, validation loss 0.67981\n",
      "Epoch 10: train loss 0.67877, validation loss 0.67857\n",
      "Epoch 11: train loss 0.67776, validation loss 0.67767\n",
      "Epoch 12: train loss 0.67747, validation loss 0.67638\n",
      "Epoch 13: train loss 0.67618, validation loss 0.67551\n",
      "Epoch 14: train loss 0.67510, validation loss 0.67469\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ds = normalized_ds\n",
    "\n",
    "# Définition de la classe OsseDataset\n",
    "class OsseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (xarray.Dataset): Doit contenir les variables \n",
    "                vozocrtxT, vomecrtyT, sossheig, votemper, eddies.\n",
    "            transform (callable, optional): Transformation à appliquer.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.dims[\"time\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extraction des variables (chaque variable est de forme (H, W))\n",
    "        U = self.data.vozocrtxT.values[idx, :, :]\n",
    "        V = self.data.vomecrtyT.values[idx, :, :]\n",
    "        sossheig = self.data.sossheig.values[idx, :, :]\n",
    "        votemper = self.data.votemper.values[idx, :, :]\n",
    "        eddies = self.data.eddies.values[idx, :, :]\n",
    "\n",
    "        # Conversion en tenseurs PyTorch\n",
    "        U = torch.tensor(U, dtype=torch.float32)\n",
    "        V = torch.tensor(V, dtype=torch.float32)\n",
    "        sossheig = torch.tensor(sossheig, dtype=torch.float32)\n",
    "        votemper = torch.tensor(votemper, dtype=torch.float32)\n",
    "        targets = torch.tensor(eddies, dtype=torch.float32)\n",
    "        \n",
    "        # Concaténation des tenseurs le long de la dimension des canaux\n",
    "        inputs_tensor = torch.stack([U, V, sossheig, votemper], dim=0)\n",
    "        \n",
    "        # Appliquer une transformation si nécessaire\n",
    "        if self.transform:\n",
    "            sample = {'inputs': inputs_tensor, 'targets': targets}\n",
    "            sample = self.transform(sample)\n",
    "            return sample['inputs'], sample['targets']\n",
    "\n",
    "        return inputs_tensor, targets\n",
    "\n",
    "# Création de l'instance du dataset\n",
    "dataset = OsseDataset(ds, transform=None)\n",
    "\n",
    "# Division en ensembles d'entraînement et de validation (80%/20%)\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "b_size = 8\n",
    "\n",
    "# Création des DataLoader pour l'entraînement et la validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=b_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=b_size, shuffle=False)\n",
    "\n",
    "\n",
    "class EddyPredictorCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EddyPredictorCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=1)  # Renvoie une carte de sortie d'un canal\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "\n",
    "# Utilisation de MSELoss pour la régression (remplaçant de CrossEntropyLoss)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=999)\n",
    "\n",
    "def validation(net):\n",
    "    net.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            labels = labels.long()\n",
    "            inputs_tensor = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #labels = labels.unsqueeze(1).to(device)  # Ajoute la dimension des canaux\n",
    "            #print(labels.shape)\n",
    "            # Prédictions et calcul de la perte\n",
    "            outputs = net(inputs_tensor)\n",
    "            #print(outputs)\n",
    "            #print(outputs.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(val_loader)  # Moyenne des pertes\n",
    "    net.train()\n",
    "    return valid_loss\n",
    "\n",
    "def train(model, num_epochs, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_history = []\n",
    "    valid_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            targets = targets.long()\n",
    "            inputs_tensor = inputs.to(device)\n",
    "            targets = targets.to(device)  # Envoi sur GPU           \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_tensor)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)  # Moyenne de la perte sur chaque image\n",
    "        valid_loss = validation(model)  # Calcul validation loss\n",
    "        train_history.append(train_loss) # On ajoute tout aux listes pour tracer les résultats \n",
    "        valid_history.append(valid_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch:02d}: train loss {train_loss:.5f}, validation loss {valid_loss:.5f}')\n",
    "    \n",
    "    torch.save(model.state_dict(), \"eddy_predictor.pth\")\n",
    "\n",
    "    return train_history, valid_history\n",
    "    \n",
    "\n",
    "# On se met sur les GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Device utilisé :\", device)\n",
    "\n",
    " \n",
    "# Initialisation du modèle\n",
    "model = EddyPredictorCNN()\n",
    "model = model.to(device) # Ajoutez les parenthèses pour instancier le modèle\n",
    "\n",
    "train_history, valid_history = train(model, 15, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'y_true' parameter of accuracy_score must be an array-like or a sparse matrix. Got EddyPredictorCNN(\n  (conv_layers): Sequential(\n    (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n  )\n) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_labels, predictions\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m plot_train_val(train_history, valid_history)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/envi1supaero/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[0;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/envi1supaero/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'y_true' parameter of accuracy_score must be an array-like or a sparse matrix. Got EddyPredictorCNN(\n  (conv_layers): Sequential(\n    (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n  )\n) instead."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "def plot_train_val(train, valid):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.set_ylabel('Training', color=color)\n",
    "    ax1.plot(train, color=color)\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Validation', color=color)\n",
    "    plt.title('Loss')\n",
    "    ax2.plot(valid, color=color)\n",
    "    fig.tight_layout()\n",
    "\n",
    "def get_valid_predictions(net, loader):\n",
    "    net.eval() # pas en mode entrainement\n",
    "    all_labels = np.array([])\n",
    "    predictions = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            input, labels = data\n",
    "            input, labels = input.to(device), labels.to(device)\n",
    "            outputs = net(input)\n",
    "            _, predicted = torch.max(outputs, 1) # Cette fois nous voulons des labels (plus la proba d'avoir chaque label), donc torch.max\n",
    "            all_labels = np.append(all_labels, labels.cpu().numpy())  # Ramène sur CPU\n",
    "            predictions = np.append(predictions, predicted.cpu().numpy())\n",
    "    return all_labels, predictions\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ', accuracy_score(model, val_loader))\n",
    "plot_train_val(train_history, valid_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi1supaero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
